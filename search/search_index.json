{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TIL Today I Learned A random collection of (hopefully daily) learnings. Inspired by https://github.com/jbranchaud/til .","title":"TIL"},{"location":"#til","text":"Today I Learned A random collection of (hopefully daily) learnings. Inspired by https://github.com/jbranchaud/til .","title":"TIL"},{"location":"django/","text":"Django Adding non-model fields when using ModelAdmin You may want to include extra (non-model) fields in the admin interface. When using ModelAdmin , you can specify the form to use. As always, you can use custom validation with a clean_<field> method on the form. class MyForm(forms.ModelForm): non_model_field_a = forms.CharField(required=False) def clean_non_model_field_a(self): # conditional logic based potentially on other fields pass class MyAdmin(admin.ModelAdmin): form = MyForm def get_fieldsets(self, request, obj=None): # conditional rendering if obj: pass else: pass def save_model(self, request, obj, form, change): special_condition = True if special_condition: non_model_field_a = form.cleaned_data['non_model_field_a'] ... Editing related objects in the admin Taken from the Django docs . Suppose you have two related models from django.db import models class Author(models.Model): name = models.CharField(max_length=100) class Book(models.Model): author = models.ForeignKey(Author, on_delete=models.CASCADE) title = models.CharField(max_length=100) You can edit the Book from the AuthorAdmin page. from django.contrib import admin class BookInline(admin.TabularInline): model = Book class AuthorAdmin(admin.ModelAdmin): inlines = [ BookInline, ] Combined with ModelAdmin.save_formset , you can perform conditional logic based on the related object you are saving. class AuthorAdmin(admin.ModelAdmin): def save_formset(self, request, form, formset, change): # save_formset called for each formset instances = formset.save(commit=False) for instance in instances: if isinstance(instance, RelatedObjectA): instance.user = request.user instance.save() formset.save_m2m() Inlines come in two flavors: StackedInline and TabularInline . You can optionally specify a formset for the inline which allows you to customize the usage of the related models. Possibilities: limit the queryset, specify whether you can add/delete, edit only certain fields etc.","title":"Django"},{"location":"django/#django","text":"","title":"Django"},{"location":"django/#adding-non-model-fields-when-using-modeladmin","text":"You may want to include extra (non-model) fields in the admin interface. When using ModelAdmin , you can specify the form to use. As always, you can use custom validation with a clean_<field> method on the form. class MyForm(forms.ModelForm): non_model_field_a = forms.CharField(required=False) def clean_non_model_field_a(self): # conditional logic based potentially on other fields pass class MyAdmin(admin.ModelAdmin): form = MyForm def get_fieldsets(self, request, obj=None): # conditional rendering if obj: pass else: pass def save_model(self, request, obj, form, change): special_condition = True if special_condition: non_model_field_a = form.cleaned_data['non_model_field_a'] ...","title":"Adding non-model fields when using ModelAdmin"},{"location":"django/#editing-related-objects-in-the-admin","text":"Taken from the Django docs . Suppose you have two related models from django.db import models class Author(models.Model): name = models.CharField(max_length=100) class Book(models.Model): author = models.ForeignKey(Author, on_delete=models.CASCADE) title = models.CharField(max_length=100) You can edit the Book from the AuthorAdmin page. from django.contrib import admin class BookInline(admin.TabularInline): model = Book class AuthorAdmin(admin.ModelAdmin): inlines = [ BookInline, ] Combined with ModelAdmin.save_formset , you can perform conditional logic based on the related object you are saving. class AuthorAdmin(admin.ModelAdmin): def save_formset(self, request, form, formset, change): # save_formset called for each formset instances = formset.save(commit=False) for instance in instances: if isinstance(instance, RelatedObjectA): instance.user = request.user instance.save() formset.save_m2m() Inlines come in two flavors: StackedInline and TabularInline . You can optionally specify a formset for the inline which allows you to customize the usage of the related models. Possibilities: limit the queryset, specify whether you can add/delete, edit only certain fields etc.","title":"Editing related objects in the admin"},{"location":"django/adding-non-model-fields-using-modeladmin/","text":"Adding non-model fields when using ModelAdmin You may want to include extra (non-model) fields in the admin interface. When using ModelAdmin , you can specify the form to use. As always, you can use custom validation with a clean_<field> method on the form. class MyForm(forms.ModelForm): non_model_field_a = forms.CharField(required=False) def clean_non_model_field_a(self): # conditional logic based potentially on other fields pass class MyAdmin(admin.ModelAdmin): form = MyForm def get_fieldsets(self, request, obj=None): # conditional rendering if obj: pass else: pass def save_model(self, request, obj, form, change): special_condition = True if special_condition: non_model_field_a = form.cleaned_data['non_model_field_a'] ...","title":"Adding non-model fields when using ModelAdmin"},{"location":"django/adding-non-model-fields-using-modeladmin/#adding-non-model-fields-when-using-modeladmin","text":"You may want to include extra (non-model) fields in the admin interface. When using ModelAdmin , you can specify the form to use. As always, you can use custom validation with a clean_<field> method on the form. class MyForm(forms.ModelForm): non_model_field_a = forms.CharField(required=False) def clean_non_model_field_a(self): # conditional logic based potentially on other fields pass class MyAdmin(admin.ModelAdmin): form = MyForm def get_fieldsets(self, request, obj=None): # conditional rendering if obj: pass else: pass def save_model(self, request, obj, form, change): special_condition = True if special_condition: non_model_field_a = form.cleaned_data['non_model_field_a'] ...","title":"Adding non-model fields when using ModelAdmin"},{"location":"django/editing-related-objects-in-admin/","text":"Editing related objects in the admin Taken from the Django docs . Suppose you have two related models from django.db import models class Author(models.Model): name = models.CharField(max_length=100) class Book(models.Model): author = models.ForeignKey(Author, on_delete=models.CASCADE) title = models.CharField(max_length=100) You can edit the Book from the AuthorAdmin page. from django.contrib import admin class BookInline(admin.TabularInline): model = Book class AuthorAdmin(admin.ModelAdmin): inlines = [ BookInline, ] Combined with ModelAdmin.save_formset , you can perform conditional logic based on the related object you are saving. class AuthorAdmin(admin.ModelAdmin): def save_formset(self, request, form, formset, change): # save_formset called for each formset instances = formset.save(commit=False) for instance in instances: if isinstance(instance, RelatedObjectA): instance.user = request.user instance.save() formset.save_m2m() Inlines come in two flavors: StackedInline and TabularInline . You can optionally specify a formset for the inline which allows you to customize the usage of the related models. Possibilities: limit the queryset, specify whether you can add/delete, edit only certain fields etc.","title":"Editing related objects in the admin"},{"location":"django/editing-related-objects-in-admin/#editing-related-objects-in-the-admin","text":"Taken from the Django docs . Suppose you have two related models from django.db import models class Author(models.Model): name = models.CharField(max_length=100) class Book(models.Model): author = models.ForeignKey(Author, on_delete=models.CASCADE) title = models.CharField(max_length=100) You can edit the Book from the AuthorAdmin page. from django.contrib import admin class BookInline(admin.TabularInline): model = Book class AuthorAdmin(admin.ModelAdmin): inlines = [ BookInline, ] Combined with ModelAdmin.save_formset , you can perform conditional logic based on the related object you are saving. class AuthorAdmin(admin.ModelAdmin): def save_formset(self, request, form, formset, change): # save_formset called for each formset instances = formset.save(commit=False) for instance in instances: if isinstance(instance, RelatedObjectA): instance.user = request.user instance.save() formset.save_m2m() Inlines come in two flavors: StackedInline and TabularInline . You can optionally specify a formset for the inline which allows you to customize the usage of the related models. Possibilities: limit the queryset, specify whether you can add/delete, edit only certain fields etc.","title":"Editing related objects in the admin"},{"location":"openwrt/custom_dns/custom_dns/","text":"OpenWRT Custom DNS Use Case I want to route all traffic for a specific interface through pi-hole. Instructions OpenWRT should be using dnsmasq . To set the DNS, we need touse custom DHCP-Options. Navigate to the interfaces page. Select the interface you would like to edit Scroll down to the DHCP section and select \"Advanced\" Set the custom DNS. We want to use option 6, so the syntax is 6,<address1>,<address2> etc. Click \"Save and Apply.\" All done!","title":"Custom DNS"},{"location":"openwrt/custom_dns/custom_dns/#openwrt-custom-dns","text":"","title":"OpenWRT Custom DNS"},{"location":"openwrt/custom_dns/custom_dns/#use-case","text":"I want to route all traffic for a specific interface through pi-hole.","title":"Use Case"},{"location":"openwrt/custom_dns/custom_dns/#instructions","text":"OpenWRT should be using dnsmasq . To set the DNS, we need touse custom DHCP-Options. Navigate to the interfaces page. Select the interface you would like to edit Scroll down to the DHCP section and select \"Advanced\" Set the custom DNS. We want to use option 6, so the syntax is 6,<address1>,<address2> etc. Click \"Save and Apply.\" All done!","title":"Instructions"},{"location":"postgres/explicit_joins_and_geqo_threshold/","text":"Explicit Joins and Genetic Query Optimization While debugging a hairy, slow query, I noticed that a difference in JOIN and conditons in the WHERE clause resulted in significantly different query times. How does this happen; isn't the query planner supposed to be smart? I read the Explicit JOIN Clauses doc and learned the following: When a query only involves two or three tables, there aren't many join orders to worry about. But the number of possible join orders grows exponentially as the number of tables expands. Beyond ten or so input tables it's no longer practical to do an exhaustive search of all the possibilities, and even for six or seven tables planning might take an annoyingly long time. When there are too many input tables, the PostgreSQL planner will switch from exhaustive search to a genetic probabilistic search through a limited number of possibilities. (The switch-over threshold is set by the geqo_threshold run-time parameter.) The genetic search takes less time, but it won't necessarily find the best possible plan. It turns out this particular query involved 13 joins. I need to investigate this further to see if tweaking this parameter really changes things. An alternative and perhaps better approach would be breaking up the query and handling some of the joining elsewhere (python, additional queries etc.)","title":"Explicit JOINs and Genetic Query Optimization"},{"location":"postgres/explicit_joins_and_geqo_threshold/#explicit-joins-and-genetic-query-optimization","text":"While debugging a hairy, slow query, I noticed that a difference in JOIN and conditons in the WHERE clause resulted in significantly different query times. How does this happen; isn't the query planner supposed to be smart? I read the Explicit JOIN Clauses doc and learned the following: When a query only involves two or three tables, there aren't many join orders to worry about. But the number of possible join orders grows exponentially as the number of tables expands. Beyond ten or so input tables it's no longer practical to do an exhaustive search of all the possibilities, and even for six or seven tables planning might take an annoyingly long time. When there are too many input tables, the PostgreSQL planner will switch from exhaustive search to a genetic probabilistic search through a limited number of possibilities. (The switch-over threshold is set by the geqo_threshold run-time parameter.) The genetic search takes less time, but it won't necessarily find the best possible plan. It turns out this particular query involved 13 joins. I need to investigate this further to see if tweaking this parameter really changes things. An alternative and perhaps better approach would be breaking up the query and handling some of the joining elsewhere (python, additional queries etc.)","title":"Explicit Joins and Genetic Query Optimization"},{"location":"postgres/slow_local_postgres_connection/","text":"Fixing Slow Local Postgres Connections tl;dr: use 127.0.0.1 instead of localhost or disable ipv6 for localhost . Seemingly out of the blue, I noticed a significant increase in query time when developing in Django. After a bit of digging around in Django's internals, I found that the database connection was taking a while (~26s) to connect. Weird. I then went through the following sequence: > psql \\c <database> select 1 from <table>; ...quick. Ok. Sanity check: > ./manage.py shell Model.objects.count() ...takes a while. Digging, digging... Github issue > export DSN=\"dbname=dbname host=localhost port=5432 user=<user> password=<pw>\" > time python -c \"import os, psycopg2; print(psycopg2.connect(os.environ['DSN']))\" I then switched the host from localhost to 127.0.0.1 which fixed the issue. Why does this happen? Well, /etc/hosts looks like this: 127.0.0.1 localhost 255.255.255.255 broadcasthost ::1 localhost localhost can use either ipv4 or ipv6. By default it seems that ipv6 is used which postgres does not like for some reason even though it says it's listening: > tail ~/Library/Application\\ Support/Postgres/var-10/postgresql.log 2019-12-04 19:32:10.774 PST [93667] LOG: worker process: logical replication launcher (PID 93674) exited with exit code 1 2019-12-04 19:32:10.774 PST [93669] LOG: shutting down 2019-12-04 19:32:10.786 PST [93667] LOG: database system is shut down 2019-12-04 19:32:11.756 PST [93969] LOG: listening on IPv6 address \"::1\", port 5432 2019-12-04 19:32:11.756 PST [93969] LOG: listening on IPv4 address \"127.0.0.1\", port 5432 2019-12-04 19:32:11.756 PST [93969] LOG: listening on Unix socket \"/tmp/.s.PGSQL.5432\" 2019-12-04 19:32:11.772 PST [93969] LOG: could not send test message on socket for statistics collector: No buffer space available 2019-12-04 19:32:11.772 PST [93969] LOG: trying another address for the statistics collector 2019-12-04 19:32:11.775 PST [93970] LOG: database system was shut down at 2019-12-04 19:32:10 PST 2019-12-04 19:32:11.803 PST [93969] LOG: database system is ready to accept connections The workaround for now is to either comment out ::1 from /etc/hosts or explicitly connect to 127.0.0.1 instead of localhost .","title":"Slow Local Postgres Connection"},{"location":"postgres/slow_local_postgres_connection/#fixing-slow-local-postgres-connections","text":"tl;dr: use 127.0.0.1 instead of localhost or disable ipv6 for localhost . Seemingly out of the blue, I noticed a significant increase in query time when developing in Django. After a bit of digging around in Django's internals, I found that the database connection was taking a while (~26s) to connect. Weird. I then went through the following sequence: > psql \\c <database> select 1 from <table>; ...quick. Ok. Sanity check: > ./manage.py shell Model.objects.count() ...takes a while. Digging, digging... Github issue > export DSN=\"dbname=dbname host=localhost port=5432 user=<user> password=<pw>\" > time python -c \"import os, psycopg2; print(psycopg2.connect(os.environ['DSN']))\" I then switched the host from localhost to 127.0.0.1 which fixed the issue. Why does this happen? Well, /etc/hosts looks like this: 127.0.0.1 localhost 255.255.255.255 broadcasthost ::1 localhost localhost can use either ipv4 or ipv6. By default it seems that ipv6 is used which postgres does not like for some reason even though it says it's listening: > tail ~/Library/Application\\ Support/Postgres/var-10/postgresql.log 2019-12-04 19:32:10.774 PST [93667] LOG: worker process: logical replication launcher (PID 93674) exited with exit code 1 2019-12-04 19:32:10.774 PST [93669] LOG: shutting down 2019-12-04 19:32:10.786 PST [93667] LOG: database system is shut down 2019-12-04 19:32:11.756 PST [93969] LOG: listening on IPv6 address \"::1\", port 5432 2019-12-04 19:32:11.756 PST [93969] LOG: listening on IPv4 address \"127.0.0.1\", port 5432 2019-12-04 19:32:11.756 PST [93969] LOG: listening on Unix socket \"/tmp/.s.PGSQL.5432\" 2019-12-04 19:32:11.772 PST [93969] LOG: could not send test message on socket for statistics collector: No buffer space available 2019-12-04 19:32:11.772 PST [93969] LOG: trying another address for the statistics collector 2019-12-04 19:32:11.775 PST [93970] LOG: database system was shut down at 2019-12-04 19:32:10 PST 2019-12-04 19:32:11.803 PST [93969] LOG: database system is ready to accept connections The workaround for now is to either comment out ::1 from /etc/hosts or explicitly connect to 127.0.0.1 instead of localhost .","title":"Fixing Slow Local Postgres Connections"},{"location":"python/auto-reload-ipython/","text":"Auto-reload in iPython I've been meaning to do this for literally years. Create an ipython profile if you do not already have one. ipython profile create Edit your profile (on osx it's at ~/.ipython/profile_default/ipython_config.py ): # These two lines will probably be commented out somewhere c.InteractiveShellApp.exec_lines = ['%autoreload 2'] c.InteractiveShellApp.extensions = ['autoreload'] Say goodbye to exiting/restarting iPython every time your code changes.","title":"Auto-reload in iPython"},{"location":"python/auto-reload-ipython/#auto-reload-in-ipython","text":"I've been meaning to do this for literally years. Create an ipython profile if you do not already have one. ipython profile create Edit your profile (on osx it's at ~/.ipython/profile_default/ipython_config.py ): # These two lines will probably be commented out somewhere c.InteractiveShellApp.exec_lines = ['%autoreload 2'] c.InteractiveShellApp.extensions = ['autoreload'] Say goodbye to exiting/restarting iPython every time your code changes.","title":"Auto-reload in iPython"},{"location":"raspberry_pi/headless_setup/","text":"Headless Raspberry Pi Setup I have a lot of raspberry pi's sitting around. Sometimes I flash new images and want to get install the same packages. I don't want to learn ansible (no idea if that would slow down the pi... probably), so here's a bunch of steps to get things up and running. Before initial boot download the image use something like Etcher or dd to flash the image cd /Volumes/boot and run touch ssh in the same directory, create a wpa_supplicant.conf file ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev country=US network={ ssid=\"<ssid>\" psk=\"<pd>\" key_mgmt=WPA-PSK } After booting sudo apt-get update && sudo apt-get upgrade && sudo apt-get dist-upgrade sudo apt-get install -y tmux vim git add gpu_mem=16 to /boot/config.txt since this will be headless follow these instructions to install zsh/oh-my-zsh clone dotiles/copy over install any necessary zsh-plugins/themes etc. follow these instructions to setup weechat Weechat setup TODO: write a small script to replace some values in the configuration files (usernames etc) with env variables. At some point I'll have config files in the dotfiles repo. Until then just scp all configs over except sec.conf . /unset weechat.network.gnutls_ca_file /secure passphrase <passphrase> to store things in sec.conf /secure set freenode_password xxxxxxx","title":"Headless Setup"},{"location":"raspberry_pi/headless_setup/#headless-raspberry-pi-setup","text":"I have a lot of raspberry pi's sitting around. Sometimes I flash new images and want to get install the same packages. I don't want to learn ansible (no idea if that would slow down the pi... probably), so here's a bunch of steps to get things up and running.","title":"Headless Raspberry Pi Setup"},{"location":"raspberry_pi/headless_setup/#before-initial-boot","text":"download the image use something like Etcher or dd to flash the image cd /Volumes/boot and run touch ssh in the same directory, create a wpa_supplicant.conf file ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev country=US network={ ssid=\"<ssid>\" psk=\"<pd>\" key_mgmt=WPA-PSK }","title":"Before initial boot"},{"location":"raspberry_pi/headless_setup/#after-booting","text":"sudo apt-get update && sudo apt-get upgrade && sudo apt-get dist-upgrade sudo apt-get install -y tmux vim git add gpu_mem=16 to /boot/config.txt since this will be headless follow these instructions to install zsh/oh-my-zsh clone dotiles/copy over install any necessary zsh-plugins/themes etc. follow these instructions to setup weechat","title":"After booting"},{"location":"raspberry_pi/headless_setup/#weechat-setup","text":"TODO: write a small script to replace some values in the configuration files (usernames etc) with env variables. At some point I'll have config files in the dotfiles repo. Until then just scp all configs over except sec.conf . /unset weechat.network.gnutls_ca_file /secure passphrase <passphrase> to store things in sec.conf /secure set freenode_password xxxxxxx","title":"Weechat setup"},{"location":"unix/exit_frozen_ssh/","text":"Exit Frozen SSH Simple trick to exit frozen ssh sessions instead of spamming Ctrl + C (which probably doesn't work anyway). Simply type ~. .","title":"Exit Frozen SSH"},{"location":"unix/exit_frozen_ssh/#exit-frozen-ssh","text":"Simple trick to exit frozen ssh sessions instead of spamming Ctrl + C (which probably doesn't work anyway). Simply type ~. .","title":"Exit Frozen SSH"},{"location":"vim/format_json/","text":"Formatting JSON in Vim Python has a built-in json.tool module which can format JSON. To invoke with vim, simple run :%!python -m json.tool .","title":"Format JSON"},{"location":"vim/format_json/#formatting-json-in-vim","text":"Python has a built-in json.tool module which can format JSON. To invoke with vim, simple run :%!python -m json.tool .","title":"Formatting JSON in Vim"}]}